{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CoolCodeMan/intro2LT/blob/Jenny/keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D195n37DW_4S",
        "outputId": "a7e88508-8270-4613-b303-bfe0a008671a"
      },
      "source": [
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import sklearn.svm\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras import optimizers\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "!wget -nc -O joy-train.tsv http://dl.turkunlp.org/TKO_8966_2021-projects/joy-annotation/train.tsv\n",
        "!wget -nc -O joy-dev.tsv http://dl.turkunlp.org/TKO_8966_2021-projects/joy-annotation/dev.tsv\n",
        "!wget -nc -O joy-test.tsv http://dl.turkunlp.org/TKO_8966_2021-projects/joy-annotation/test.tsv"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-29 07:36:03--  http://dl.turkunlp.org/TKO_8966_2021-projects/joy-annotation/train.tsv\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 71887 (70K) [application/octet-stream]\n",
            "Saving to: ‘joy-train.tsv’\n",
            "\n",
            "joy-train.tsv       100%[===================>]  70.20K   300KB/s    in 0.2s    \n",
            "\n",
            "2021-04-29 07:36:04 (300 KB/s) - ‘joy-train.tsv’ saved [71887/71887]\n",
            "\n",
            "--2021-04-29 07:36:04--  http://dl.turkunlp.org/TKO_8966_2021-projects/joy-annotation/dev.tsv\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9697 (9.5K) [application/octet-stream]\n",
            "Saving to: ‘joy-dev.tsv’\n",
            "\n",
            "joy-dev.tsv         100%[===================>]   9.47K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-04-29 07:36:04 (160 MB/s) - ‘joy-dev.tsv’ saved [9697/9697]\n",
            "\n",
            "--2021-04-29 07:36:04--  http://dl.turkunlp.org/TKO_8966_2021-projects/joy-annotation/test.tsv\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19663 (19K) [application/octet-stream]\n",
            "Saving to: ‘joy-test.tsv’\n",
            "\n",
            "joy-test.tsv        100%[===================>]  19.20K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-04-29 07:36:04 (164 KB/s) - ‘joy-test.tsv’ saved [19663/19663]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-v4iks8XKFM"
      },
      "source": [
        "with open(\"joy-train.tsv\", \"r\") as f:\n",
        "  joy_train = pd.read_csv((\"joy-train.tsv\"), header=0, names=['annotation','text'], sep='\\t') # eli tässä luen .tsv-filun ja tallennan sen tuonne joy_trainiin, käyttäen pandas-kirjastoa (importattu as pd)\n",
        "\n",
        "# tässä voi kattoa miltä nyt näyttää toi joy_train\n",
        "# print(joy_train[:10])\n",
        "# print(type(joy_train))\n",
        "\n",
        "with open(\"joy-dev.tsv\", \"r\") as f:\n",
        "  joy_dev = pd.read_csv((\"joy-dev.tsv\"), header=0, names=['annotation','text'], sep='\\t') # sama dev-teksteille\n",
        "\n",
        "# seuraavaksi luen tekstit ja niiden annotationit omiin listoihinsa molempien (train ja dev) osalta. Tässä vois käyttää tietysti list comprehensionia mut hlökoht pidän tätä \"auki kirjoitettua\" for-loopia\n",
        "# helpompana ymmärtää!\n",
        "\n",
        "train_texts = []\n",
        "for i in joy_train[\"text\"]:\n",
        "  train_texts.append(i)\n",
        "train_labels = []\n",
        "for i in joy_train[\"annotation\"]:\n",
        "  train_labels.append(i)\n",
        "\n",
        "dev_texts = []\n",
        "for i in joy_dev[\"text\"]:\n",
        "  dev_texts.append(i)\n",
        "dev_labels = []\n",
        "for i in joy_dev[\"annotation\"]:\n",
        "  dev_labels.append(i)\n",
        "\n",
        "# tarkistetaan että molemmissa on tekstejä yhtä paljon kuin labeleitä\n",
        "# assert on nopeampi/lyhyempi tapa tehdä käytännössä if True, continue / if False, raise exception -tyyppinen tarkastus\n",
        "\n",
        "\n",
        "assert len(train_texts) == len(train_labels)\n",
        "assert len(dev_texts) == len(dev_labels)\n",
        "\n",
        "#tässä voi kattella mitä nyt nuo listat pitää sisällään\n",
        "\n",
        "#for label, text in list(zip(train_labels, train_texts))[:20]:\n",
        "  #print(label,text)\n",
        "#for label, text in list(zip(dev_labels, dev_texts))[:20]:\n",
        "  #print(label,text)\n",
        "\n",
        "# en näitä nyt shufflaa missään välissä, pitäisikö? luentovideolla perusteltiin datan shufflausta silloin, kun siitä datasta aiotaan erottaa train/dev/test -data (jotta ei käy niin et esim kun ottaa ekat\n",
        "# 30 % trainiin niin ne on kaikki yhtä luokkaa jne) mutta meillä on tässä valmiiksi erotellut train/dev/test, joten onko tarpeen shufflata?"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DvKwyTlXOOz",
        "outputId": "526eb3f9-7f21-4a54-9661-385676f9af91"
      },
      "source": [
        "# laskeskelen tässä, minkä verran train datassa on kumpaakin labelia\n",
        "not_joys = train_labels.count(\"not-joy\")\n",
        "joys = len(train_labels)-not_joys\n",
        "percent_joy = (joys/len(train_labels))*100\n",
        "percent_not = 100-percent_joy\n",
        "\n",
        "print(not_joys,joys)\n",
        "print(percent_not, percent_joy)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "543 414\n",
            "56.739811912225704 43.260188087774296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_vwd5tFXdfF"
      },
      "source": [
        "vectorizer=TfidfVectorizer(binary=True) # määritellään vectorizeriksi sklearnin TfidVectorizer näillä parametreillä. binary=True tekee sen, että kaikki term frequencyt jotka on yli 0 on 1, eli\n",
        "                                        # ei oteta huomioon sitä, kuinka monta kertaa termi on dokumentissa, vaan sillä on väliä, onko se vai eikö se ole -> eli binäärinen systeemi.\n",
        "\n",
        "# sitten tehdään sekä train että dev teksteistä omat feature matrixit käyttäen tätä vectorizeria (featureiden ollessa nyt siis käytännössä sanoja).\n",
        "# train-teksteihin käytetään fit_transformia, jonka avulla vectorizer oppii sanaston ja idf:t ja palauttaa document-term matriisin.\n",
        "# koska train-tekstien avulla on tehty jo fit eli opetus, deviin riittää pelkkä transform (joka palauttaa document-term matriisin)\n",
        "\n",
        "feature_matrix_train=vectorizer.fit_transform(train_texts)\n",
        "feature_matrix_dev=vectorizer.transform(dev_texts)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2U2IGDbXWqq"
      },
      "source": [
        "# sitten ois vuorossa NN-based BoW classifier\n",
        "\n",
        "# keras ei hyväksy numpy matriiseja, joten ne pitää muuttaa tensor-matriiseiksi, luennolla opet selitti löytäneensä\n",
        "# ja kopioineensa tän stack overflowsta ja hyvin näyttää toimivan\n",
        "\n",
        "def convert_sparse_matrix_to_sparse_tensor(X):\n",
        "    coo = X.tocoo()\n",
        "    indices = np.mat([coo.row, coo.col]).transpose()\n",
        "    return tf.sparse.reorder(tf.SparseTensor(indices, coo.data, coo.shape))\n",
        "\n",
        "feature_matrix_train_tf=convert_sparse_matrix_to_sparse_tensor(feature_matrix_train)\n",
        "feature_matrix_dev_tf=convert_sparse_matrix_to_sparse_tensor(feature_matrix_dev)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kftrgHc_Xj-q",
        "outputId": "87525f87-a7e2-4d62-d7b6-f5bb25ab1db2"
      },
      "source": [
        "# sitten muutetaan labelit numeroiksi label_encoderin avulla. Ensin train labeleihin fit_transform joka sovittaa labelit numeoriksi ja palauttaa muutetut,\n",
        "# dev-labeleille ei tarvitse taas fittiä enää tehdä vaan transform riittää\n",
        "\n",
        "label_encoder=LabelEncoder() \n",
        "class_numbers_train=label_encoder.fit_transform(train_labels)\n",
        "class_numbers_dev=label_encoder.transform(dev_labels)\n",
        "\n",
        "print(\"class_numbers shape=\",class_numbers_train.shape)\n",
        "print(\"class labels\",label_encoder.classes_) #this will let us translate back from indices to labels"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "class_numbers shape= (957,)\n",
            "class labels ['joy' 'not-joy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEueOvl6Xp48",
        "outputId": "699fdd9b-7b4b-44e1-d69e-1904be5188d8"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import initializers\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import Adam\n",
        "from keras.activations import softmax\n",
        "\n",
        "from keras import callbacks\n",
        "earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", \\\n",
        "                                        mode =\"min\", patience = 5, \\\n",
        "                                        restore_best_weights = True)\n",
        "\n",
        "example_count,feature_count=feature_matrix_train.shape\n",
        "example_count2=class_numbers_train.shape[0]\n",
        "assert example_count==example_count2  # sanity check\n",
        "\n",
        "class_count=len(label_encoder.classes_) # luokkien määrä, tässä vois olla kaksi, mutta näin voidaan käyttää samaa koodia muuallakin\n",
        "\n",
        "# rakennetaan neuraaliverkko:\n",
        "\n",
        "inp=Input(shape=(feature_count,)) \n",
        "hidden=Dense(1500, bias_regularizer=l2(0.0000001), activation=\"tanh\")(inp) \n",
        "hidden2=Dropout(0.8)(hidden)\n",
        "hidden3=Dense(1500, bias_regularizer=l2(0.0000001), activation=\"relu\")(hidden2)\n",
        "hidden4=Dropout(0.5)(hidden3)\n",
        "outp=Dense(class_count,activation='sigmoid')(hidden4) #softplus #softmax #sigmoid\n",
        "\n",
        "model=Model(inputs=[inp], outputs=[outp])\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss=\"sparse_categorical_crossentropy\", metrics=\"accuracy\")\n",
        "\n",
        "hist=model.fit(feature_matrix_train_tf,class_numbers_train,\\\n",
        "               validation_data=(feature_matrix_dev_tf,class_numbers_dev), #callbacks = [earlystopping], \\\n",
        "               batch_size=8,verbose=1,epochs=1000)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 41ms/step - loss: 0.6903 - accuracy: 0.5375 - val_loss: 0.6762 - val_accuracy: 0.5694\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6689 - accuracy: 0.5755 - val_loss: 0.6559 - val_accuracy: 0.6181\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6449 - accuracy: 0.6138 - val_loss: 0.6232 - val_accuracy: 0.6667\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 0.5797 - accuracy: 0.7098 - val_loss: 0.5721 - val_accuracy: 0.6806\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.4749 - accuracy: 0.8389 - val_loss: 0.5322 - val_accuracy: 0.7083\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 0.3526 - accuracy: 0.9136 - val_loss: 0.5371 - val_accuracy: 0.7431\n",
            "Epoch 7/1000\n",
            "107/120 [=========================>....] - ETA: 0s - loss: 0.2138 - accuracy: 0.9610"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOwqiN9dXw0D"
      },
      "source": [
        "print(hist.history[\"val_accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAkS2YcNXyVM"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.ylim(0.55,1.0)\n",
        "plt.plot(hist.history[\"val_accuracy\"],label=\"Validation set accuracy\")\n",
        "plt.plot(hist.history[\"accuracy\"],label=\"Training set accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}